{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'new file'"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def data_extract(path_to_db):\n",
    "    # connect to the database\n",
    "    conn = sqlite3.connect(path_to_db)\n",
    "\n",
    "    query = \"\"\"\n",
    "        SELECT\n",
    "        e.\"Customer Identity\",\n",
    "        \"First PolicyÂ´s Year\",\n",
    "        \"Brithday Year\",\n",
    "        \"Educational Degree\",\n",
    "        \"Gross Monthly Salary\",\n",
    "        \"Geographic Living Area\",\n",
    "        \"Has Children (Y=1)\",\n",
    "        \"Customer Monetary Value\",\n",
    "        \"Claims Rate\",\n",
    "        l.\"Premiums in LOB: Motor\",\n",
    "        l.\"Premiums in LOB: Household\",\n",
    "        l.\"Premiums in LOB: Health\",\n",
    "        l.\"Premiums in LOB:  Life\",\n",
    "        l.\"Premiums in LOB: Work Compensations\"\n",
    "        FROM\n",
    "        Engage AS e\n",
    "        JOIN LOB AS l ON l.\"Customer Identity\" = e.\"Customer Identity\"\n",
    "        ORDER BY\n",
    "        e.\"Customer Identity\";\n",
    "    \"\"\"\n",
    "\n",
    "    data_df = pd.read_sql_query(query, conn)\n",
    "    df = data_df.copy()  # let's keep a copy of the original data\n",
    "\n",
    "    # remaining column names to manageable variable names\n",
    "    column_names = ['ID', 'First_Policy', 'Birthday', 'Education',\n",
    "                    'Salary', 'Area', 'Children', 'CMV', 'Claims',\n",
    "                    'Motor', 'Household', 'Health', 'Life',\n",
    "                    'Work_Compensation']\n",
    "    # renaming the columns\n",
    "    df.columns = column_names\n",
    "    # seting 'ID' as index\n",
    "    df.set_index('ID', inplace=True, drop=True)\n",
    "    return data_df, df\n",
    "\n",
    "\n",
    "my_path = r'/home/kalrashid/Dropbox/nova/data_mining/project/data/insurance.db'\n",
    "_, df = data_extract(my_path)\n",
    "\n",
    "\n",
    "\"\"\"new file\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the data \n",
    "\n",
    "my_path = r'./data/insurance.db' #path of the data file\n",
    "_, df = data_extract(my_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>First_Policy</th>\n",
       "      <th>Birthday</th>\n",
       "      <th>Education</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Area</th>\n",
       "      <th>Children</th>\n",
       "      <th>CMV</th>\n",
       "      <th>Claims</th>\n",
       "      <th>Motor</th>\n",
       "      <th>Household</th>\n",
       "      <th>Health</th>\n",
       "      <th>Life</th>\n",
       "      <th>Work_Compensation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>1982.0</td>\n",
       "      <td>2 - High School</td>\n",
       "      <td>2177.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>380.97</td>\n",
       "      <td>0.39</td>\n",
       "      <td>375.85</td>\n",
       "      <td>79.45</td>\n",
       "      <td>146.36</td>\n",
       "      <td>47.01</td>\n",
       "      <td>16.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1981.0</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>2 - High School</td>\n",
       "      <td>677.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-131.13</td>\n",
       "      <td>1.12</td>\n",
       "      <td>77.46</td>\n",
       "      <td>416.20</td>\n",
       "      <td>116.69</td>\n",
       "      <td>194.48</td>\n",
       "      <td>106.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>1970.0</td>\n",
       "      <td>1 - Basic</td>\n",
       "      <td>2277.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>504.67</td>\n",
       "      <td>0.28</td>\n",
       "      <td>206.15</td>\n",
       "      <td>224.50</td>\n",
       "      <td>124.58</td>\n",
       "      <td>86.35</td>\n",
       "      <td>99.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>1981.0</td>\n",
       "      <td>3 - BSc/MSc</td>\n",
       "      <td>1099.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-16.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>182.48</td>\n",
       "      <td>43.35</td>\n",
       "      <td>311.17</td>\n",
       "      <td>35.34</td>\n",
       "      <td>28.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1986.0</td>\n",
       "      <td>1973.0</td>\n",
       "      <td>3 - BSc/MSc</td>\n",
       "      <td>1763.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.23</td>\n",
       "      <td>0.90</td>\n",
       "      <td>338.62</td>\n",
       "      <td>47.80</td>\n",
       "      <td>182.59</td>\n",
       "      <td>18.78</td>\n",
       "      <td>41.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    First_Policy  Birthday        Education  Salary  Area  Children     CMV  \\\n",
       "ID                                                                            \n",
       "1         1985.0    1982.0  2 - High School  2177.0   1.0       1.0  380.97   \n",
       "2         1981.0    1995.0  2 - High School   677.0   4.0       1.0 -131.13   \n",
       "3         1991.0    1970.0        1 - Basic  2277.0   3.0       0.0  504.67   \n",
       "4         1990.0    1981.0      3 - BSc/MSc  1099.0   4.0       1.0  -16.99   \n",
       "5         1986.0    1973.0      3 - BSc/MSc  1763.0   4.0       1.0   35.23   \n",
       "\n",
       "    Claims   Motor  Household  Health    Life  Work_Compensation  \n",
       "ID                                                                \n",
       "1     0.39  375.85      79.45  146.36   47.01              16.89  \n",
       "2     1.12   77.46     416.20  116.69  194.48             106.13  \n",
       "3     0.28  206.15     224.50  124.58   86.35              99.02  \n",
       "4     0.99  182.48      43.35  311.17   35.34              28.34  \n",
       "5     0.90  338.62      47.80  182.59   18.78              41.45  "
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the data\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "First_Policy          30\n",
       "Birthday              17\n",
       "Education             17\n",
       "Salary                36\n",
       "Area                   1\n",
       "Children              21\n",
       "CMV                    0\n",
       "Claims                 0\n",
       "Motor                 34\n",
       "Household              0\n",
       "Health                43\n",
       "Life                 104\n",
       "Work_Compensation     86\n",
       "dtype: int64"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sb\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "\"\"\"\n",
    "Steps to follow, according to the lectures:\n",
    "Data preparation\n",
    " Exploratory data analysis\n",
    " Detecting outliers\n",
    " Dealing with missing values\n",
    " Data discretization\n",
    " Imbalanced learning and data generation\n",
    "\n",
    "Data preprocessing\n",
    " The curse of dimensionality\n",
    " Identifying informative attributes/features\n",
    " Creating attributes/features\n",
    " Dimensionality reduction\n",
    "  Relevancy\n",
    "  Redundancy\n",
    " Data standardization\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# undocumented handy function: df._get_numeric_data()\n",
    "\n",
    "\n",
    "def cleaning_df(df):\n",
    "    # removing duplicate rows\n",
    "    df = df[~df.duplicated(keep=\"last\")]\n",
    "    # turning impossible values into NaN\n",
    "    df.loc[df[\"Birthday\"] < 1900, \"Birthday\"] = np.nan\n",
    "    df.loc[df[\"Birthday\"] > 2016, \"Birthday\"] = np.nan\n",
    "    df.loc[df[\"First_Policy\"] > 2016, \"First_Policy\"] = np.nan\n",
    "    df.loc[df[\"Birthday\"] > df[\"First_Policy\"], \"First_Policy\"] = np.nan\n",
    "\n",
    "    # turning Education into numeric\n",
    "    df[\"Education\"] = df[\"Education\"].str.extract(r\"(\\d)\").astype(np.float)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_dummies(df, cols):\n",
    "    \"\"\"Adds dummy columns to selected variables using the One Hot Encoding method.\n",
    "    Drops the first column.\"\"\"\n",
    "    df_with_dummies = pd.get_dummies(df, columns=cols, drop_first=True)\n",
    "    return df_with_dummies\n",
    "\n",
    "\n",
    "def outlier_conditions(df):\n",
    "    \"\"\"\n",
    "    Sets the condition for the identification of outliers in a dataframe\n",
    "    \"\"\"\n",
    "    # ~((df < (Q1 - 1.5 * IQR)) |(df > (Q3 + 1.5 * IQR)))\n",
    "\n",
    "\n",
    "    # Q1 = df['col'].quantile(.25)\n",
    "    # Q3 = df['col'].quantile(.75)\n",
    "    # mask = d['col'].between(q1, q2, inclusive=True)\n",
    "    # iqr = d.loc[mask, 'col']\n",
    "\n",
    "\n",
    "\n",
    "    return ~(np.abs(df - df.mean()) > (3 * df.std()))\n",
    "\n",
    "\n",
    "def remove_outliers(df, cols):\n",
    "    \"\"\"\n",
    "    Replaces outliers by NaNs.\n",
    "    Selected columns must be numerical.\n",
    "    \"\"\"\n",
    "    outlier_df_cond = outlier_conditions(df)\n",
    "    outliers_count = (\n",
    "        (df[cols] == df[outlier_df_cond][cols]) == False\n",
    "        )[cols].sum()\n",
    "    \n",
    "    temp_df = df[cols].copy()\n",
    "    outlier_tempdf_cond = outlier_conditions(temp_df)\n",
    "    temp_df = temp_df[outlier_tempdf_cond]\n",
    "    \n",
    "    df.loc[:, cols] = temp_df.loc[:, cols].copy()\n",
    "    return df, outliers_count\n",
    "\n",
    "\n",
    "# def remove_outliers(df, cols):\n",
    "#     \"\"\"\n",
    "#     Replaces outliers by NaNs.\n",
    "#     Selected columns must be numerical.\n",
    "#     \"\"\"\n",
    "#     ~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR)))\n",
    "\n",
    "\n",
    "#     for col in cols:\n",
    "#         Q1 = df[col].quantile(.25)\n",
    "#         Q3 = df[col].quantile(.75)\n",
    "\n",
    "#         mask = df[col].between(Q1, Q3, inclusive=True)\n",
    "#         IQR = df.loc[mask, col]\n",
    "        \n",
    "#         df.loc[\n",
    "#             (df[col] < (Q1 - 1.5 * IQR)) | ( )\n",
    "#             , col\n",
    "#         ] = np.nan\n",
    "\n",
    "#         cond = (df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))\n",
    "\n",
    "\n",
    "\n",
    "#     return df, outliers_count\n",
    "\n",
    "\n",
    "def handle_nans(df, cols):\n",
    "    \"\"\"\n",
    "    Replaces NaNs by column mean.\n",
    "    Selected columns must be continuous.\n",
    "    \"\"\"\n",
    "    df.fillna(df.mean()[cols], inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def handle_premium_nans(df, cols):\n",
    "    \"\"\"\n",
    "    Replaces NaNs with 0.\n",
    "    Selected columns must be continuous.\n",
    "    \"\"\"\n",
    "    for col in cols:\n",
    "        df[col].fillna(0, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def handle_cat_nans(df, cat_cols, independent_cols):\n",
    "    \"\"\"\n",
    "    Uses a Random Forest classifier to predict and impute the nan values \n",
    "    for each categorical column given in `cols`.\n",
    "    \"\"\"\n",
    "    \n",
    "    Xcols, imputated_cols = [], []\n",
    "\n",
    "    for cat_col in cols:\n",
    "        if df[cat_col].isna().any().sum() > 0:\n",
    "            Xcols.append(cat_col)\n",
    "    \n",
    "    if len(Xcols) > 0:\n",
    "        for nan_col in Xcols:\n",
    "            X_train = df.loc[:, df.columns.difference(list(set(Xcols) - set(imputated_cols)))].values\n",
    "            y_train = df.loc[:, nan_col].values\n",
    "\n",
    "            # TODO: tune Random Forest\n",
    "            clf = RandomForestClassifier(n_estimators=200, max_depth=5, random_state=2019)\n",
    "            clf.fit(X_train, y_train)\n",
    "\n",
    "            X_test = df.loc[df[cat_col].isna(), Xcols].copy()\n",
    "            no_of_nans = len(X_test)\n",
    "\n",
    "            y_pred = clf.predict(X_test)\n",
    "            \n",
    "            for pred, index in zip(y_pred, X_test.index.tolist()):\n",
    "                df.loc[index, cat_col] = pred\n",
    "\n",
    "            imputated_cols.append(cat_col)\n",
    "            print(f'{no_of_nans} NaN values of \"{cat_col}\" column were imputed.')\n",
    "        return df\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "\n",
    "def standardize_data(df, cols):\n",
    "    \"\"\"Standardizes data from `cols`.\n",
    "    cols -> list\n",
    "    \"\"\"\n",
    "    df_Norm = df[cols].copy()\n",
    "    df_Norm[cols] = StandardScaler().fit_transform(df[cols])\n",
    "    return df, df_Norm\n",
    "\n",
    "\n",
    "def feature_selection(df):\n",
    "    corr = df.corr(method='pearson')\n",
    "\n",
    "    # Obtain Correlation and plot it\n",
    "    plt.figure(figsize=(16,6))\n",
    "\n",
    "    h_map = sb.heatmap(corr, \n",
    "            xticklabels=corr.columns,\n",
    "            yticklabels=corr.columns,\n",
    "            cmap='PRGn', annot=True, linewidths=.5)\n",
    "\n",
    "    bottom, top = h_map.get_ylim()\n",
    "    h_map.set_ylim(bottom + 0.5, top - 0.5)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def feature_eng(df):\n",
    "    \"\"\"\n",
    "    Creates useful features from the original ones in the dataframe.\n",
    "    \"\"\"\n",
    "    if \"Birthday\" in df.columns:\n",
    "        df[\"Age\"] = 2016 - df[\"Birthday\"]\n",
    "        del df[\"Birthday\"]\n",
    "    \n",
    "    if \"First_Policy\" in df.columns:\n",
    "        df[\"Customer_Years\"] = 2016 - df[\"First_Policy\"]\n",
    "        del df[\"First_Policy\"]\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def dim_reduction(df):\n",
    "    \"\"\"\n",
    "    Applies Principal Component Analysis (PCA) to the dataframe.\n",
    "    \"\"\"\n",
    "    x = df.values\n",
    "    pca = PCA(n_components=2)\n",
    "    principalComponents = pca.fit_transform(x)\n",
    "    principalDf = pd.DataFrame(data = principalComponents, columns = ['principal_comp_1', 'principal_comp_2'])\n",
    "    print(pca.explained_variance_ratio_)\n",
    "    return principalDf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoded_education = pd.get_dummies(df['Education'], prefix = 'Education')    \n",
    "one_hot_encoded_area = pd.get_dummies(df['Area'], prefix = 'Area')\n",
    "one_hot_encoded_children = pd.get_dummies(df['Children'], prefix = 'Children')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10296"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(one_hot_encoded_area)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "df05 = df.drop(['Area', 'Children', 'Education'], axis=1)\n",
    "\n",
    "df1 = pd.concat([df05, one_hot_encoded_area, one_hot_encoded_children, one_hot_encoded_education], axis=1)\n",
    "\n",
    "len(df)\n",
    "#len(df1)\n",
    "df1.reset_index(drop=True)\n",
    "df1.head()\n",
    "\n",
    "df1.index = np.arange(0, len(df1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10296"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()\n",
    "df1.index = np.arange(0, len(df1))\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no = scaler.fit_transform(df1)\n",
    "df_no = pd.DataFrame(df_no, columns = df1.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10296"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10296"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-303-0aedc6f76131>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mkm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mkm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mSum_of_squared_distances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minertia_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    970\u001b[0m                 \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m                 return_n_iter=True)\n\u001b[0m\u001b[1;32m    973\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36mk_means\u001b[0;34m(X, n_clusters, sample_weight, init, precompute_distances, n_init, max_iter, verbose, tol, random_state, copy_x, n_jobs, algorithm, return_n_iter)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0morder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"C\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcopy_x\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     X = check_array(X, accept_sparse='csr', dtype=[np.float64, np.float32],\n\u001b[0;32m--> 312\u001b[0;31m                     order=order, copy=copy_x)\n\u001b[0m\u001b[1;32m    313\u001b[0m     \u001b[0;31m# verify that the number of samples given is larger than k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 542\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "\n",
    "Sum_of_squared_distances = []\n",
    "K = range(1,20)\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(df_no)\n",
    "    Sum_of_squared_distances.append(km.inertia_)\n",
    "\n",
    "# Plot the elbow\n",
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def handle_cat_nans(df, cols):\n",
    "    \"\"\"\n",
    "    Uses a Random Forest classifier to predict and impute the nan values \n",
    "    for each categorical column given in `cols`.\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Xcols, imputated_cols = [], []\n",
    "\n",
    "    for cat_col in cols:\n",
    "        if df[cat_col].isna().any().sum() > 0:\n",
    "            Xcols.append(cat_col)\n",
    "    \n",
    "    if len(Xcols) > 0:\n",
    "        for nan_col in Xcols:\n",
    "            X_train = df.loc[:, df.columns.difference(list(set(Xcols) - set(imputated_cols)))].values\n",
    "            y_train = df.loc[:, nan_col].values\n",
    "\n",
    "            # TODO: tune Random Forest\n",
    "            clf = RandomForestClassifier(n_estimators=200, max_depth=5, random_state=2019)\n",
    "            clf.fit(X_train, y_train)\n",
    "\n",
    "            X_test = df.loc[df[cat_col].isna(), Xcols].copy()\n",
    "            no_of_nans = len(X_test)\n",
    "\n",
    "            y_pred = clf.predict(X_test)\n",
    "            \n",
    "            for pred, index in zip(y_pred, X_test.index.tolist()):\n",
    "                df.loc[index, cat_col] = pred\n",
    "\n",
    "            imputated_cols.append(cat_col)\n",
    "            print(f'{no_of_nans} NaN values of \"{cat_col}\" column were imputed.')\n",
    "        return df\n",
    "    else:\n",
    "        return df\n",
    "        \n",
    "        titanicWithAge = titanic[pd.isnull(titanic['age']) == False]\n",
    "        titanicWithoutAge = titanic[pd.isnull(titanic['age'])]\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    variables = cols\n",
    "    \n",
    "    #for col in cols:\n",
    "    one_hot_encoded_area = pd.get_dummies(df['Area'])\n",
    "    one_hot_encoded_education = pd.get_dummies(df['Education'])\n",
    "    one_hot_encoded_children = pd.get_dummies(df['Children'])\n",
    "    \n",
    "    customer_with_area = df[pd.isnull(df['Area']) == False]\n",
    "    customer_without_area = df[pd.isnull(df['Area'])]\n",
    "    \n",
    "    customer_with_edu = df[pd.isnull(df['Education']) == False]\n",
    "    customer_without_edu = df[pd.isnull(df['Education'])]\n",
    "\n",
    "    customer_with_child = df[pd.isnull(df['Children']) == False]\n",
    "    customer_without_child = df[pd.isnull(df['Children'])]\n",
    "    \n",
    "    \n",
    "    customer_with_area = df[variables]\n",
    "    customer_with_area = pd.concat([customer_with_area, one_hot_encoded_children, one_hot_encoded_education], axis =1)\n",
    "    \n",
    "    customer_without_area = customer_without_area[variables]\n",
    "    customer_without_area = pd.concat([customer_without_area, one_hot_encoded_children, one_hot_encoded_education], axis = 1)\n",
    "    \n",
    "\n",
    "    customer_with_edu = df[variables]\n",
    "    customer_with_edu = pd.concat([customer_with_edu, one_hot_encoded_area, one_hot_encoded_education], axis =1)\n",
    "    \n",
    "    customer_without_edu = customer_without_area[variables]\n",
    "    customer_without_edu = pd.concat([customer_without_edu, one_hot_encoded_area, one_hot_encoded_education], axis = 1)\n",
    "    \n",
    "    customer_with_child = df[variables]\n",
    "    customer_with_child = pd.concat([customer_with_child, one_hot_encoded_children, one_hot_encoded_area], axis =1)\n",
    "    \n",
    "    customer_without_child = customer_without_area[variables]\n",
    "    customer_without_child = pd.concat([customer_without_child, one_hot_encoded_children, one_hot_encoded_area], axis = 1)\n",
    "    \n",
    "    independentVariables = ['pclass', 'female', 'male', 'sibsp', 'parch', 'fare', 'C', 'Q', 'S']\n",
    "\n",
    "    rfModel_area = RandomForestRegressor()\n",
    "    rfModel_area.fit(titanicWithAge[independentVariables], df['Area'])\n",
    "\n",
    "    generatedAgeValues = rfModel_age.predict(X = titanicWithoutAge[independentVariables])\n",
    "    \n",
    "    \n",
    "    \n",
    "    titanicWithAge = titanicWithAge[variables]\n",
    "    titanicWithAge = pd.concat([titanicWithAge, one_hot_encoded_sex, one_hot_encoded_embarked], axis = 1)\n",
    "\n",
    "    one_hot_encoded_embarked = pd.get_dummies(titanicWithoutAge['embarked'])\n",
    "    one_hot_encoded_sex = pd.get_dummies(titanicWithoutAge['sex'])\n",
    "    titanicWithoutAge = titanicWithoutAge[variables]\n",
    "    titanicWithoutAge = pd.concat([titanicWithoutAge, one_hot_encoded_sex, one_hot_encoded_embarked], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def handle_cats(df, cat_cols, independent_cols):\n",
    "    \"\"\"\n",
    "    Uses a Random Forest classifier to predict and impute the nan values \n",
    "    for each categorical column given in `cols`.\n",
    "    \"\"\"\n",
    "    \n",
    "    Xcols, ind_cols, imputated_cols = [], [], []\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    for cat_col in cat_cols:\n",
    "        if df[cat_col].isna().any().sum() > 0:\n",
    "            Xcols.append(cat_col)\n",
    "    \n",
    "\n",
    "    \n",
    "    if len(Xcols) > 0:\n",
    "        for nan_col in Xcols:\n",
    "            #create dataframe\n",
    "            temp_df = df[[*independent_cols, nan_col]]\n",
    "            temp_df_non_nan = temp_df.dropna()\n",
    "            \n",
    "            X_train = temp_df_non_nan.loc[:,independent_cols]\n",
    "            y_train = temp_df_non_nan.loc[:, nan_col]\n",
    "            \n",
    "            #print(y_train)\n",
    "\n",
    "            # TODO: tune Random Forest\n",
    "            clf = RandomForestClassifier(n_estimators=200, max_depth=5, random_state=2019)\n",
    "            clf.fit(X_train, y_train)\n",
    "            \n",
    "            #X_test = temp_df.copy()\n",
    "            X_test = temp_df[independent_cols]\n",
    "\n",
    "            print(f'Xtrain-length {len(X_train)}, X-test leng: {len(X_test)}')\n",
    "            \n",
    "            \n",
    "            #print(X_test.isna().any())\n",
    "            #print(df.loc[df[cat_col].isna(), independent_cols].head())\n",
    "            no_of_nans = len(X_test)\n",
    "\n",
    "\n",
    "\n",
    "            y_pred = clf.predict(X_test)\n",
    "            \n",
    "            for pred, index in zip(y_pred, X_test.index.tolist()):\n",
    "                df.loc[index, cat_col] = pred\n",
    "\n",
    "            imputated_cols.append(cat_col)\n",
    "            print(f'{no_of_nans} NaN values of \"{cat_col}\" column were imputed.')\n",
    "        return df\n",
    "    else:\n",
    "        return df\n",
    "        #\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def handle_cats1(df, cat_cols, independent_cols):\n",
    "    \"\"\"\n",
    "    Uses a Random Forest classifier to predict and impute the nan values \n",
    "    for each categorical column given in `cols`.\n",
    "    \"\"\"\n",
    "    print(independent_cols)\n",
    "    \n",
    "    Xcols, imputated_cols = [], []\n",
    "\n",
    "    for cat_col in cat_cols:\n",
    "        if df[cat_col].isna().any().sum() > 0:\n",
    "            Xcols.append(cat_col)\n",
    "    \n",
    "    if len(Xcols) > 0:\n",
    "        for nan_col in Xcols:\n",
    "            #X_train = df.loc[:, df.columns.difference(list(set(independent_cols) - set(Xcols)))].values\n",
    "            temp_df = df.copy()\n",
    "            \n",
    "            temp_df[nan_col].fillna(df[nan_col].mode()[0], inplace=True)\n",
    "\n",
    "            X_train = temp_df.loc[:, independent_cols].values\n",
    "            y_train = temp_df.loc[:, nan_col].values\n",
    "            #print(df.loc[:, independent_cols])\n",
    "            # TODO: tune Random Forest\n",
    "            clf = RandomForestClassifier(n_estimators=200, max_depth=5, random_state=2019)\n",
    "            clf.fit(X_train, y_train)\n",
    "            #print(df.loc[df[cat_col].isna()])\n",
    "            X_test = temp_df.loc[:, independent_cols].copy()\n",
    "            print(X_test.isna().any())\n",
    "            no_of_nans = len(X_test)\n",
    "\n",
    "            y_pred = clf.predict(X_test)\n",
    "            \n",
    "            for pred, index in zip(y_pred, X_test.index.tolist()):\n",
    "                df.loc[index, cat_col] = pred\n",
    "\n",
    "            imputated_cols.append(cat_col)\n",
    "            print(f'{no_of_nans} NaN values of \"{cat_col}\" column were imputed.')\n",
    "        return df\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Salary', 'CMV', 'Motor', 'Household', 'Health', 'Life', 'Work_Compensation', 'Customer_Years']\n",
      "Salary               False\n",
      "CMV                  False\n",
      "Motor                False\n",
      "Household            False\n",
      "Health               False\n",
      "Life                 False\n",
      "Work_Compensation    False\n",
      "Customer_Years       False\n",
      "dtype: bool\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kalrashid/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10293 NaN values of \"Education\" column were imputed.\n",
      "Salary               False\n",
      "CMV                  False\n",
      "Motor                False\n",
      "Household            False\n",
      "Health               False\n",
      "Life                 False\n",
      "Work_Compensation    False\n",
      "Customer_Years       False\n",
      "dtype: bool\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kalrashid/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot setitem on a Categorical with a new category, set the categories first",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-273-26c19ce4d6a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_cats1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindependent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-272-cdfe1d2e9fd1>\u001b[0m in \u001b[0;36mhandle_cats1\u001b[0;34m(df, cat_cols, independent_cols)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_col\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mimputated_cols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m    555\u001b[0m                 \u001b[0;31m# scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m                     \u001b[0msetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36msetter\u001b[0;34m(item, v)\u001b[0m\n\u001b[1;32m    488\u001b[0m                     \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m                     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m                     \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m                     \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_update_cacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36msetitem\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"setitem\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mputmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)\u001b[0m\n\u001b[1;32m    436\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malign_copy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36msetitem\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m   1836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1837\u001b[0m         \u001b[0mcheck_setitem_lengths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1838\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1839\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_add\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_add\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2189\u001b[0m             raise ValueError(\n\u001b[0;32m-> 2190\u001b[0;31m                 \u001b[0;34m\"Cannot setitem on a Categorical with a new \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2191\u001b[0m                 \u001b[0;34m\"category, set the categories first\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2192\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot setitem on a Categorical with a new category, set the categories first"
     ]
    }
   ],
   "source": [
    "new_df = handle_cats1(df, cat, independent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Education</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Area</th>\n",
       "      <th>Children</th>\n",
       "      <th>CMV</th>\n",
       "      <th>Claims</th>\n",
       "      <th>Motor</th>\n",
       "      <th>Household</th>\n",
       "      <th>Health</th>\n",
       "      <th>Life</th>\n",
       "      <th>Work_Compensation</th>\n",
       "      <th>Age</th>\n",
       "      <th>Customer_Years</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2177.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>380.97</td>\n",
       "      <td>0.39</td>\n",
       "      <td>375.85</td>\n",
       "      <td>79.45</td>\n",
       "      <td>146.36</td>\n",
       "      <td>47.01</td>\n",
       "      <td>16.89</td>\n",
       "      <td>34.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>677.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-131.13</td>\n",
       "      <td>1.12</td>\n",
       "      <td>77.46</td>\n",
       "      <td>416.20</td>\n",
       "      <td>116.69</td>\n",
       "      <td>0.00</td>\n",
       "      <td>106.13</td>\n",
       "      <td>21.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2277.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>504.67</td>\n",
       "      <td>0.28</td>\n",
       "      <td>206.15</td>\n",
       "      <td>224.50</td>\n",
       "      <td>124.58</td>\n",
       "      <td>86.35</td>\n",
       "      <td>99.02</td>\n",
       "      <td>46.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1099.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-16.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>182.48</td>\n",
       "      <td>43.35</td>\n",
       "      <td>311.17</td>\n",
       "      <td>35.34</td>\n",
       "      <td>28.34</td>\n",
       "      <td>35.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1763.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.23</td>\n",
       "      <td>0.90</td>\n",
       "      <td>338.62</td>\n",
       "      <td>47.80</td>\n",
       "      <td>182.59</td>\n",
       "      <td>18.78</td>\n",
       "      <td>41.45</td>\n",
       "      <td>43.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Education  Salary Area Children     CMV  Claims   Motor  Household  Health  \\\n",
       "ID                                                                              \n",
       "1        2.0  2177.0  1.0      1.0  380.97    0.39  375.85      79.45  146.36   \n",
       "2        2.0   677.0  4.0      1.0 -131.13    1.12   77.46     416.20  116.69   \n",
       "3        1.0  2277.0  3.0      0.0  504.67    0.28  206.15     224.50  124.58   \n",
       "4        3.0  1099.0  4.0      1.0  -16.99    0.99  182.48      43.35  311.17   \n",
       "5        3.0  1763.0  4.0      1.0   35.23    0.90  338.62      47.80  182.59   \n",
       "\n",
       "     Life  Work_Compensation   Age  Customer_Years  \n",
       "ID                                                  \n",
       "1   47.01              16.89  34.0            31.0  \n",
       "2    0.00             106.13  21.0            29.0  \n",
       "3   86.35              99.02  46.0            25.0  \n",
       "4   35.34              28.34  35.0            26.0  \n",
       "5   18.78              41.45  43.0            30.0  "
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = ['Area', 'Children', 'Education']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Area         True\n",
       "Children     True\n",
       "Education    True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[cat].isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "independent = ['Salary', 'CMV', 'Motor', 'Household', 'Health', 'Life', 'Work_Compensation', 'Customer_Years']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Salary               0\n",
       "CMV                  0\n",
       "Motor                0\n",
       "Household            0\n",
       "Health               0\n",
       "Life                 0\n",
       "Work_Compensation    0\n",
       "Customer_Years       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[independent].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Area  Education  Children\n",
      "ID                              \n",
      "1       1.0        2.0       1.0\n",
      "2       4.0        2.0       1.0\n",
      "3       3.0        1.0       0.0\n",
      "4       4.0        3.0       1.0\n",
      "5       4.0        3.0       1.0\n",
      "...     ...        ...       ...\n",
      "10292   2.0        4.0       0.0\n",
      "10293   3.0        1.0       0.0\n",
      "10294   1.0        3.0       1.0\n",
      "10295   2.0        1.0       1.0\n",
      "10296   1.0        4.0       1.0\n",
      "\n",
      "[10293 rows x 3 columns]\n",
      "First_Policy          0\n",
      "Birthday              0\n",
      "Education            17\n",
      "Salary                0\n",
      "Area                  1\n",
      "Children             21\n",
      "CMV                   0\n",
      "Claims                0\n",
      "Motor                 0\n",
      "Household             0\n",
      "Health                0\n",
      "Life                  0\n",
      "Work_Compensation     0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kalrashid/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n",
      "/home/kalrashid/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/kalrashid/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "/home/kalrashid/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:85: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/kalrashid/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py:6287: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/kalrashid/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/kalrashid/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:202: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/kalrashid/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:206: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Motor                False\n",
       "Household            False\n",
       "Health               False\n",
       "Life                 False\n",
       "Work_Compensation    False\n",
       "Salary               False\n",
       "CMV                  False\n",
       "Customer_Years       False\n",
       "Area                  True\n",
       "Education             True\n",
       "Children              True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def preprocessing_df(df):\n",
    "    #separation of variables\n",
    "    ValueEngage = ['Age', 'Education', 'Salary', 'Area', 'Children', 'CMV', 'Customer_Years']\n",
    "\n",
    "    ConsAff = ['Motor', 'Household', 'Health', 'Life', 'Work_Compensation']\n",
    "    Cat_Values = [\"Area\", \"Education\", \"Children\"]\n",
    "    \n",
    "    collist = []\n",
    "    collist.extend(ConsAff)\n",
    "    collist.extend(Cat_Values)\n",
    "\n",
    "    df = cleaning_df(df)\n",
    "    df, outliers_count = remove_outliers(df, df.columns)\n",
    "    \n",
    "    df = handle_nans(df, df.columns.difference(collist))\n",
    "    df = handle_premium_nans(df, ConsAff)\n",
    "    print(df[Cat_Values])\n",
    "    print(df.isna().sum())\n",
    "    #df = handle_cat_nans(df, Cat_Values)\n",
    "\n",
    "    df.loc[:, [\"First_Policy\", \"Birthday\", \"Salary\"]] = df[[\"First_Policy\", \"Birthday\", \"Salary\"]].round().astype(np.int32)\n",
    "\n",
    "    df.loc[:, Cat_Values] = df[Cat_Values].astype(\"category\")\n",
    "    \n",
    "    df = feature_eng(df)\n",
    "    df, df_Norm = standardize_data(df, [*ConsAff, 'Salary', 'CMV', 'Customer_Years'])\n",
    "\n",
    "    # df = dim_reduction(df)\n",
    "    \n",
    "    return df, df_Norm\n",
    "\n",
    "\n",
    "\n",
    "df, df_Norm = preprocessing_df(df)\n",
    "\n",
    "df_Norm['Area'], df_Norm['Education'], df_Norm['Children'] = df['Area'], df['Education'], df['Children']\n",
    "\n",
    "df_Norm.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
